{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create SNN Model"
      ],
      "metadata": {
        "id": "pIDdqy0JMdQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install packages**"
      ],
      "metadata": {
        "id": "gNYEV2yxNCod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "!pip install git+https://github.com/miladmozafari/SpykeTorch.git\n",
        "import SpykeTorch.snn as snn\n",
        "import SpykeTorch.functional as sf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsvyrbpaMg89",
        "outputId": "2e4128bd-a5dd-4b7e-b678-a8f4126e0668"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/miladmozafari/SpykeTorch.git\n",
            "  Cloning https://github.com/miladmozafari/SpykeTorch.git to /tmp/pip-req-build-u0jnfilv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/miladmozafari/SpykeTorch.git /tmp/pip-req-build-u0jnfilv\n",
            "  Resolved https://github.com/miladmozafari/SpykeTorch.git to commit aa302c77cb61de6ec64e40927313137a9855ec6a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from SpykeTorch-miladmozafari==0.0.1) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from SpykeTorch-miladmozafari==0.0.1) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from SpykeTorch-miladmozafari==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from SpykeTorch-miladmozafari==0.0.1) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SpykeTorch-miladmozafari==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SpykeTorch-miladmozafari==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->SpykeTorch-miladmozafari==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->SpykeTorch-miladmozafari==0.0.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create DCSNN model**"
      ],
      "metadata": {
        "id": "Z5tIEKQMPNbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCSNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DCSNN, self).__init__()\n",
        "\n",
        "    #(in_channels, out_channels, kernel_size, weight_mean=0.8, weight_std=0.02)\n",
        "    self.conv1 = snn.Convolution(6, 30, 5, 0.8, 0.05)\n",
        "    self.conv2 = snn.Convolution(30, 250, 3, 0.8, 0.05)\n",
        "    self.conv3 = snn.Convolution(250, 200, 5, 0.8, 0.05)\n",
        "\n",
        "    #(conv_layer, learning_rate, use_stabilizer=True, lower_bound=0, upper_bound=1)\n",
        "    self.stdp1 = snn.STDP(self.conv1, (0.004, -0.003))\n",
        "    self.stdp2 = snn.STDP(self.conv2, (0.004, -0.003))\n",
        "    self.stdp3 = snn.STDP(self.conv3, (0.004, -0.003), False, 0.2, 0.8)\n",
        "    self.anti_stdp3 = snn.STDP(self.conv3, (0.004, -0.0005), False, 0.2, 0.8)\n",
        "\n",
        "    def save_data(self, input_spk, pot, spk, winners):\n",
        "      self.ctx[\"input_spikes\"] = input_spk\n",
        "      self.ctx[\"potentials\"] = pot\n",
        "      self.ctx[\"output_spikes\"] = spk\n",
        "      self.ctx[\"winners\"] = winners\n",
        "\n",
        "    def forward(self, input, max_layer):\n",
        "      input = sf.pad(input, (2, 2, 2, 2))\n",
        "      if self.training: # forward pass for training\n",
        "        pot = self.conv1(input)\n",
        "        spk, pot = sf.fire(pot, 15, True)\n",
        "        if max_layer == 1:\n",
        "          winners = sf.get_k_winners(pot, 5, 3)\n",
        "          self.save_data(input, pot, spk, winners)\n",
        "          return spk, pot\n",
        "        spk_in = sf.pad(sf.pooling(spk, 2, 2), (1, 1, 1, 1))\n",
        "        pot = self.conv2(spk_in)\n",
        "        spk, pot = sf.fire(pot, 10, True)\n",
        "        if max_layer == 2:\n",
        "          winners = sf.get_k_winners(pot, 8, 2)\n",
        "          self.save_data(spk_in, pot, spk, winners)\n",
        "        spk_in = sf.pad(sf.pooling(spk, 3, 3), (2, 2, 2, 2))\n",
        "        pot = self.conv3(spk_in)\n",
        "        spk = sf.fire_(pot)\n",
        "        winners = sf.get_k_winners(pot, 1)\n",
        "        self.save_data(spk_in, pot, spk, winners)\n",
        "        output = -1\n",
        "        if len(winners) != 0:\n",
        "          output = self.decision_map[winners[0][0]]\n",
        "        return output\n",
        "      else: # forward pass for testing\n",
        "        pot = self.conv1(input)\n",
        "        spk = sf.fire(pot, 15)\n",
        "        pot = self.conv2(sf.pad(sf.pooling(spk, 2, 2), (1, 1, 1, 1)))\n",
        "        spk = sf.fire(pot, 10)\n",
        "        pot = self.conv3(sf.pad(sf.pooling(spk, 3, 3), (2, 2, 2, 2)))\n",
        "        # omitting the threshold parameter means infinite threshold\n",
        "        spk = sf.fire_(pot)\n",
        "        winners = sf.get_k_winners(pot, 1)\n",
        "        output = -1\n",
        "        # each winner is a tuple of form (feature, row, column)\n",
        "        if len(winners) != 0:\n",
        "          output = self.decision_map[winners[0][0]]\n",
        "        return output\n",
        "\n",
        "      def stdp(self, layer_idx):\n",
        "        if layer_idx == 1:\n",
        "          self.stdp1(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "        if layer_idx == 2:\n",
        "          self.stdp2(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "\n",
        "      def reward(self):\n",
        "        self.stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "\n",
        "      def punish(self):\n",
        "        self.anti_stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])"
      ],
      "metadata": {
        "id": "U7RvGNuSNG5Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transform input images into spike waves**"
      ],
      "metadata": {
        "id": "8ojFajcuaKv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import SpykeTorch.utils as utils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class InputTransform:\n",
        "  def __init__(self, filter):\n",
        "    self.to_tensor = transforms.ToTensor()\n",
        "    self.filter = filter\n",
        "    self.temporal_transform = utils.Intensity2Latency(15, to_spike=True)\n",
        "\n",
        "  def __call__(self, image):\n",
        "    image = self.to_tensor(image) * 255 # convert image to tensor\n",
        "    image.unsqueeze_(0) # add 1 extra time dimension\n",
        "    image = self.filter(image) # apply filter\n",
        "    image = sf.local_normalization(image, 8) # normalize\n",
        "    return self.temporal_transform(image) # generate spike wave tensor\n",
        "\n",
        "kernels = [utils.DoGKernel(3, 3/9, 6/9), utils.DoGKernel(3, 6/9, 3/9),\n",
        "           utils.DoGKernel(7, 7/9, 14/9), utils.DoGKernel(7, 14/9, 7/9),\n",
        "           utils.DoGKernel(13, 13/9, 26/9), utils.DoGKernel(13, 26/9, 13/9)]\n",
        "filter = utils.Filter(kernels, padding=6, thresholds=50)\n",
        "transform = InputTransform(filter)"
      ],
      "metadata": {
        "id": "zsrstV9naKUY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare MNIST Dataset"
      ],
      "metadata": {
        "id": "JZy2vw8tbtl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "data_root = \"./\"\n",
        "\n",
        "MNIST_train = utils.CacheDataset(MNIST(root=data_root, train=True, download=True, transform=transform))\n",
        "MNIST_test = utils.CacheDataset(MNIST(root=data_root, train=False, download=True, transform=transform))\n",
        "MNIST_loader = DataLoader(MNIST_train, batch_size=1000)\n",
        "MNIST_test_loader = DataLoader(MNIST_test, batch_size=len(MNIST_test))"
      ],
      "metadata": {
        "id": "acwnFChNbwEt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test"
      ],
      "metadata": {
        "id": "HKsrEkjwc5us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised learning for normal STDP layers**"
      ],
      "metadata": {
        "id": "rfv15YlkdGRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "def train_unsupervised(network, data, layer_idx):\n",
        "  network.train()\n",
        "  for i in range(len(data)):\n",
        "    data_in = data[i].cuda() if use_cuda else data[i]\n",
        "    network(data_in, layer_idx)\n",
        "    network.stdp(layer_idx)"
      ],
      "metadata": {
        "id": "ZHr1x3-Dc65d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reinforcement learning for R-STDP layer(s)**"
      ],
      "metadata": {
        "id": "xHaSMJ14eMQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def train_rl(network, data, target):\n",
        "  network.train()\n",
        "  perf = np.array([0, 0, 0]) # correct, wrong, silent\n",
        "  for i in range(len(data)):\n",
        "    data_in = data[i].cuda() if use_cuda else data[i]\n",
        "    target_in = target[i].cuda() if use_cuda else target[i]\n",
        "    d = network(data_in, 3)\n",
        "    if d != -1:\n",
        "      if d == target_in:\n",
        "        perf[0] += 1\n",
        "        network.reward()\n",
        "      else:\n",
        "        perf[1] += 1\n",
        "        network.punish()\n",
        "    else:\n",
        "      perf[2] += 1\n",
        "  return perf / len(data)"
      ],
      "metadata": {
        "id": "t6rDRifKeL4L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and test the network**"
      ],
      "metadata": {
        "id": "mHkxQsbW7mt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = DCSNN()\n",
        "\n",
        "if use_cuda:\n",
        "  net.cuda()\n",
        "\n",
        "epochs_1 = 10\n",
        "epochs_2 = 10\n",
        "epochs_3 = 10\n",
        "\n",
        "# first layer\n",
        "for epoch in range(epochs_1):\n",
        "  for data, targets in MNIST_loader:\n",
        "    train_unsupervised(net, data, 1)\n",
        "\n",
        "# second layer\n",
        "for epoch in range(epochs_2):\n",
        "  for data, targets in MNIST_loader:\n",
        "    train_unsupervised(net, data, 2)\n",
        "\n",
        "# third layer\n",
        "for epoch in range(epochs_3):\n",
        "  for data, targets in MNIST_loader: # training\n",
        "    print(train_rl(net, data, targets))\n",
        "  for data, targets in MNIST_test_loader: # testing\n",
        "    print(test(net, data, targets))"
      ],
      "metadata": {
        "id": "sby2f7js7oCg",
        "outputId": "bae42178-9b83-4966-81ed-78b0448ef750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Module [DCSNN] is missing the required \"forward\" function",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8d9b56964fd3>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMNIST_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# second layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ae4170126b28>\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[0;34m(network, data, layer_idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Module [{type(self).__name__}] is missing the required \"forward\" function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [DCSNN] is missing the required \"forward\" function"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kheradpisheh Model"
      ],
      "metadata": {
        "id": "zhatb85P_2fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################\n",
        "# Reimplementation of the Digit Recognition Experiment (MNIST) Performed in:      #\n",
        "# https://www.sciencedirect.com/science/article/pii/S0893608017302903             #\n",
        "#                                                                                 #\n",
        "# Reference:                                                                      #\n",
        "# Kheradpisheh, Saeed Reza, et al.                                                #\n",
        "# \"STDP-based spiking deep convolutional neural networks for object recognition.\" #\n",
        "# Neural Networks 99 (2018): 56-67.                                               #\n",
        "#                                                                                 #\n",
        "###################################################################################\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.nn.parameter import Parameter\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from SpykeTorch import snn\n",
        "from SpykeTorch import functional as sf\n",
        "from SpykeTorch import visualization as vis\n",
        "from SpykeTorch import utils\n",
        "from torchvision import transforms\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "class KheradpishehMNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KheradpishehMNIST, self).__init__()\n",
        "\n",
        "        self.conv1 = snn.Convolution(2, 32, 5, 0.8, 0.05)\n",
        "        self.conv1_t = 10\n",
        "        self.k1 = 5\n",
        "        self.r1 = 2\n",
        "\n",
        "        self.conv2 = snn.Convolution(32, 150, 2, 0.8, 0.05)\n",
        "        self.conv2_t = 1\n",
        "        self.k2 = 8\n",
        "        self.r2 = 1\n",
        "\n",
        "        self.stdp1 = snn.STDP(self.conv1, (0.004, -0.003))\n",
        "        self.stdp2 = snn.STDP(self.conv2, (0.004, -0.003))\n",
        "        self.max_ap = Parameter(torch.Tensor([0.15]))\n",
        "\n",
        "        self.ctx = {\"input_spikes\":None, \"potentials\":None, \"output_spikes\":None, \"winners\":None}\n",
        "        self.spk_cnt1 = 0\n",
        "        self.spk_cnt2 = 0\n",
        "\n",
        "    def save_data(self, input_spike, potentials, output_spikes, winners):\n",
        "        self.ctx[\"input_spikes\"] = input_spike\n",
        "        self.ctx[\"potentials\"] = potentials\n",
        "        self.ctx[\"output_spikes\"] = output_spikes\n",
        "        self.ctx[\"winners\"] = winners\n",
        "\n",
        "    def forward(self, input, max_layer):\n",
        "        input = sf.pad(input.float(), (2,2,2,2), 0)\n",
        "        if self.training:\n",
        "            pot = self.conv1(input)\n",
        "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
        "            if max_layer == 1:\n",
        "                self.spk_cnt1 += 1\n",
        "                if self.spk_cnt1 >= 500:\n",
        "                    self.spk_cnt1 = 0\n",
        "                    ap = torch.tensor(self.stdp1.learning_rate[0][0].item(), device=self.stdp1.learning_rate[0][0].device) * 2\n",
        "                    ap = torch.min(ap, self.max_ap)\n",
        "                    an = ap * -0.75\n",
        "                    self.stdp1.update_all_learning_rate(ap.item(), an.item())\n",
        "                pot = sf.pointwise_inhibition(pot)\n",
        "                spk = pot.sign()\n",
        "                winners = sf.get_k_winners(pot, self.k1, self.r1, spk)\n",
        "                self.save_data(input, pot, spk, winners)\n",
        "                return spk, pot\n",
        "            spk_in = sf.pad(sf.pooling(spk, 2, 2, 1), (1,1,1,1))\n",
        "            spk_in = sf.pointwise_inhibition(spk_in)\n",
        "            pot = self.conv2(spk_in)\n",
        "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
        "            if max_layer == 2:\n",
        "                pot = sf.pointwise_inhibition(pot)\n",
        "                spk = pot.sign()\n",
        "                winners = sf.get_k_winners(pot, self.k2, self.r2, spk)\n",
        "                self.save_data(spk_in, pot, spk, winners)\n",
        "                return spk, pot\n",
        "            spk_out = sf.pooling(spk, 2, 2, 1)\n",
        "            return spk_out\n",
        "        else:\n",
        "            pot = self.conv1(input)\n",
        "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
        "            pot = self.conv2(sf.pad(sf.pooling(spk, 2, 2, 1), (1,1,1,1)))\n",
        "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
        "            spk = sf.pooling(spk, 2, 2, 1)\n",
        "            return spk\n",
        "\n",
        "    def stdp(self, layer_idx):\n",
        "        if layer_idx == 1:\n",
        "            self.stdp1(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "        if layer_idx == 2:\n",
        "            self.stdp2(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"])\n",
        "\n",
        "def train_unsupervise(network, data, layer_idx):\n",
        "    network.train()\n",
        "    for i in range(len(data)):\n",
        "        data_in = data[i]\n",
        "        if use_cuda:\n",
        "            data_in = data_in.cuda()\n",
        "        network(data_in, layer_idx)\n",
        "        network.stdp(layer_idx)\n",
        "\n",
        "def test(network, data, target, layer_idx):\n",
        "    network.eval()\n",
        "    ans = [None] * len(data)\n",
        "    t = [None] * len(data)\n",
        "    for i in range(len(data)):\n",
        "        data_in = data[i]\n",
        "        if use_cuda:\n",
        "            data_in = data_in.cuda()\n",
        "        output,_ = network(data_in, layer_idx).max(dim = 0)\n",
        "        ans[i] = output.reshape(-1).cpu().numpy()\n",
        "        t[i] = target[i]\n",
        "    return np.array(ans), np.array(t)\n",
        "\n",
        "class S1Transform:\n",
        "    def __init__(self, filter, timesteps = 15):\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.filter = filter\n",
        "        self.temporal_transform = utils.Intensity2Latency(timesteps)\n",
        "        self.cnt = 0\n",
        "    def __call__(self, image):\n",
        "        if self.cnt % 1000 == 0:\n",
        "            print(self.cnt)\n",
        "        self.cnt+=1\n",
        "        image = self.to_tensor(image) * 255\n",
        "        image.unsqueeze_(0)\n",
        "        image = self.filter(image)\n",
        "        image = sf.local_normalization(image, 8)\n",
        "        temporal_image = self.temporal_transform(image)\n",
        "        return temporal_image.sign().byte()\n",
        "\n",
        "kernels = [ utils.DoGKernel(7,1,2),\n",
        "            utils.DoGKernel(7,2,1),]\n",
        "filter = utils.Filter(kernels, padding = 3, thresholds = 50)\n",
        "s1 = S1Transform(filter)\n",
        "\n",
        "data_root = \"data\"\n",
        "MNIST_train = utils.CacheDataset(torchvision.datasets.MNIST(root=data_root, train=True, download=True, transform = s1))\n",
        "MNIST_test = utils.CacheDataset(torchvision.datasets.MNIST(root=data_root, train=False, download=True, transform = s1))\n",
        "MNIST_loader = DataLoader(MNIST_train, batch_size=len(MNIST_train), shuffle=False)\n",
        "MNIST_testLoader = DataLoader(MNIST_test, batch_size=len(MNIST_test), shuffle=False)\n",
        "\n",
        "kheradpisheh = KheradpishehMNIST()\n",
        "if use_cuda:\n",
        "    kheradpisheh.cuda()\n",
        "\n",
        "# Training The First Layer\n",
        "print(\"Training the first layer\")\n",
        "if os.path.isfile(\"saved_l1.net\"):\n",
        "    kheradpisheh.load_state_dict(torch.load(\"saved_l1.net\"))\n",
        "else:\n",
        "    for epoch in range(2):\n",
        "        print(\"Epoch\", epoch)\n",
        "        iter = 0\n",
        "        for data,_ in MNIST_loader:\n",
        "            print(\"Iteration\", iter)\n",
        "            train_unsupervise(kheradpisheh, data, 1)\n",
        "            print(\"Done!\")\n",
        "            iter+=1\n",
        "    torch.save(kheradpisheh.state_dict(), \"saved_l1.net\")\n",
        "\n",
        "# Training The Second Layer\n",
        "print(\"Training the second layer\")\n",
        "if os.path.isfile(\"saved_l2.net\"):\n",
        "    kheradpisheh.load_state_dict(torch.load(\"saved_l2.net\"))\n",
        "for epoch in range(20):\n",
        "    print(\"Epoch\", epoch)\n",
        "    iter = 0\n",
        "    for data,_ in MNIST_loader:\n",
        "        print(\"Iteration\", iter)\n",
        "        train_unsupervise(kheradpisheh, data, 2)\n",
        "        print(\"Done!\")\n",
        "        iter+=1\n",
        "torch.save(kheradpisheh.state_dict(), \"saved_l2.net\")\n",
        "\n",
        "# Classification\n",
        "# Get train data\n",
        "for data,target in MNIST_loader:\n",
        "    train_X, train_y = test(kheradpisheh, data, target, 2)\n",
        "\n",
        "\n",
        "# Get test data\n",
        "for data,target in MNIST_testLoader:\n",
        "    test_X, test_y = test(kheradpisheh, data, target, 2)\n",
        "\n",
        "# SVM\n",
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC(C=2.4)\n",
        "clf.fit(train_X, train_y)\n",
        "predict_train = clf.predict(train_X)\n",
        "predict_test = clf.predict(test_X)\n",
        "\n",
        "def get_performance(X, y, predictions):\n",
        "    correct = 0\n",
        "    silence = 0\n",
        "    for i in range(len(predictions)):\n",
        "        if X[i].sum() == 0:\n",
        "            silence += 1\n",
        "        else:\n",
        "            if predictions[i] == y[i]:\n",
        "                correct += 1\n",
        "    return (correct/len(X), (len(X)-(correct+silence))/len(X), silence/len(X))\n",
        "\n",
        "print(get_performance(train_X, train_y, predict_train))\n",
        "print(get_performance(test_X, test_y, predict_test))"
      ],
      "metadata": {
        "id": "hssUbnUJ_1_n",
        "outputId": "8299a0cc-1737-461d-b5bd-bf4556df7d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17892323.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 474504.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4463446.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3760467.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Training the first layer\n",
            "Epoch 0\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 1\n",
            "Iteration 0\n",
            "Done!\n",
            "Training the second layer\n",
            "Epoch 0\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 1\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 2\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 3\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 4\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 5\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 6\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 7\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 8\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 9\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 10\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 11\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 12\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 13\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 14\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 15\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 16\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 17\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 18\n",
            "Iteration 0\n",
            "Done!\n",
            "Epoch 19\n",
            "Iteration 0\n",
            "Done!\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}